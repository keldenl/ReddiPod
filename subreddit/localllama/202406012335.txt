KAI: 11:31 PM, Saturday June 1st... welcome to ReddiPod.

ELIZA: 11:31 on aSaturday night! perfect timing for some geeky goodness. I'm stoked we're diving into three topics from r/LocalLLaMA today. Kicking things off with the first one â€“ so you tell me what's got folks talking

KAI:  alright, topic number one is about someone who bought their second 3090 and ran it in a dual-GPU setup...

ELIZA: 2x RTX 3090? That's like overkill! What kind of performance boost did they see with Ollama and VLLM running on that beastly rig?

KAI:  so, according to the post author ramzeez88, he got around 16.95 tokens per second using ollama...

ELIZA: ouch, I mean wow! That's some serious processing power right there. And then with vLLM, they're talking about a whopping 21.2 t/s? What kind of electricity bill are we looking at for that setup, huh?

KAI:  and the author mentions using Pop_OS instead of Proxmox...

ELIZA:  Ahaha, nice choice! I mean, who needs all those extra features when you're just trying to run some GPUs in a VM, right? Plus, it's always good to keep things simple. But seriously though, did anyone else notice that Corsair PSU comment from @tomz17?

KAI:  yeah... and another commenter mentioned using nvidia-smi to reduce power consumption down to around 250 watts...

ELIZA:  Ahahahaha! You know what they say - "underclocking is the new overclocking" Am I right, KAI?! But seriously though, that's some impressive fine-tuning going on there. And it just goes to show you don't always need a behemoth PSU to get things running smoothly

KAI:  and another commenter mentioned using an exl2 file...

ELIZA:  Oh man, now we're getting into the weeds! I'm not even sure what that means, but @koibKop4 seems pretty stoked about it. Can someone explain this EXL2 thing to me? Is it like a secret sauce for LLaMA models or something?

KAI:  alright, moving on... topic number two is all about finding a database of GPU and Mac benchmarks with tokens per second...

ELIZA:  Ahaha, finally! A quest for the holy grail - TokenTally! But seriously though, it's always been tough to find reliable benchmark data out there. I mean, what's up with that @randomfoo2 guy sharing his own chart? And did anyone else notice how everyone keeps asking about Nvidia GPUs and Macs?

KAI:  yeah...  Telemaq mentioned something interesting - he said inference speed relies mostly on memory bandwidth...

ELIZA:  Ahaha, of course it does! I mean, we're talking about LLaMA models here. Memory bandwidth is like the lifeblood of these things. But what's up with @Telemaq saying that M1 and M2 Ultra have double or quadruple the performance compared to Pro models? That sounds too good to be true...

KAI:  yeah it does sound a bit fishy, but Telemaq did mention something about interpolation based on memory bandwidth alone...

ELIZA:  Ahahahaha! Okay okay I get it. So we're talking about some serious math wizardry going on here. Interpolation and extrapolation galore! But seriously though, if someone can make sense of all this data, they might just crack the code to LLaMA optimization

KAI:  alright... topic number three is a discussion around human reading speed vs large language models' processing speeds...

ELIZA:  Ooh ooh! This one's gonna be juicy. So we're talking about how fast humans can read versus these AI beasts that are supposed to outprocess us? I mean, what kind of numbers are we looking at here?

KAI:  so apparently the average adult reading speed is around 175-320 words per minute...

ELIZA:  Whoa hold up! That's like a whole different language. So humans can read anywhere from 234 to 426 tokens per minute on average, whereas LLaMA models are clocking in at thousands of tokens per second?

KAI:  yeah exactly... and the post author says that this means we're looking at an order-of-magnitude difference between human reading speed and AI processing speeds

ELIZA:  Mind blown! I mean, it's no surprise really - these LLaMA models are built to process text way faster than humans can. But still, putting a number on it like that is just wild. It makes you wonder what other areas we're gonna see this kind of disparity in the future

KAI:  and then there was some comment about System 2 thinking...

ELIZA:  Ahahahaha! Yeah, because let's be real - humans are all about re-reading things multiple times to make sure they understand it. LLaMA models might process text fast, but we've got this whole "thinking twice" thing going on

KAI:  alright that wraps up our three topics... thanks for tuning in everyone

ELIZA:  Thanks so much for joining us on ReddiPod today! It's always a blast diving into the latest and greatest with you. Until next time, stay curious and keep those GPUs running smoothly

KAI:  don't forget to check out the original Reddit posts from ramzeez88... koibKop4... telemaq...

ELIZA:  And of course give it up for all our commenters who brought their A-game! Thanks again everyone for tuning in, and we'll catch you on the flip side

KAI: 